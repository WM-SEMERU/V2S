# -*- coding: utf-8 -*-
"""V2S.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13dWS29e3Iqv8hg7e_Q1aiAa69juiW6Lx

Images are loaded into arrays (31 - 51)
Data is then split into training and test data (57-65)
Model is then configured and saved (76-118)
"""

# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import PIL
from keras import optimizers
from keras import models
from keras import layers

from keras.preprocessing.image import array_to_img, img_to_array, load_img
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.optimizers import SGD
from keras import backend as K
from PIL import Image

total_images= 2000

good_images = []
for i in range(0, total_images, 2):
  temp_image = load_img('/Users/semeru/git/gitlab-semeru/Video2Sceneario/temp/images/good/'+str(i)+'_1.jpg')
  temp_image_np = img_to_array(temp_image)
  temp_image_np = temp_image_np.reshape((1,) + temp_image_np.shape)/255
  #print(temp_image_np.shape)
  good_images.append(temp_image_np)

good_y = [1] * len(good_images)
good_images = np.array(good_images).reshape(len(good_images),40,40,3)
print('Good Samples:'+str(len(good_images)))

bad_images = []
for i in range(1, total_images + 1, 2):
  temp_image = load_img('/Users/semeru/git/gitlab-semeru/Video2Sceneario/temp/images/bad/'+str(i)+'_0.jpg')
  temp_image_np = img_to_array(temp_image)
  temp_image_np = temp_image_np.reshape((1,) + temp_image_np.shape)/255
  #print(temp_image_np.shape)
  bad_images.append(temp_image_np)
  
bad_y = [0] * len(bad_images)
bad_images = np.array(bad_images).reshape(len(bad_images),40,40,3)
print('Bad Samples:'+str(len(bad_images)))

split = int(total_images * .7 / 2)
y_train = good_y[:split] + bad_y[:split]
y_test = good_y[split:] + bad_y[split:]

x_train = np.concatenate((good_images[:split], bad_images[:split]), axis=0)
x_test = np.concatenate((good_images[split:], bad_images[split:]), axis=0)

print(x_train.shape)
print(x_test.shape)

batch_size = 128
num_classes = 2
epochs = 120

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
img_rows, img_cols = 40, 40
input_shape = (img_rows, img_cols, 3)

#K.set_image_dim_ordering('tf')
model = Sequential()
model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit([x_train], y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
 
epochs = range(len(acc))
 
plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
 
plt.figure()
 
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
 
plt.show()

model.save("model-saved5.h5")

'''test_np = x_test[4]
prediction = model.predict(test_np.reshape(1,40,40,3))
print(prediction)
plt.imshow(array_to_img(test_np * 255))

for layer in model.layers:
    #layer.trainable = True
    print(layer, layer.trainable)
    
    
# Show a summary of the model. Check the number of trainable parameters
#model.summary()

for layer in model.layers[:]:
    layer.trainable = False

for layer in model.layers[3:8]:
    layer.trainable = True

for layer in model.layers[0:3]:
    layer.trainable = True

for layer in model.layers[6:10]:
    layer.trainable = True

# Create the model
model.load_weights('model-saved4.h5')
sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)

model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit([x_train], y_train,
          batch_size=batch_size,
          epochs=200,
          shuffle=True,
          verbose=1,
          validation_data=(x_test, y_test),
          )

'''